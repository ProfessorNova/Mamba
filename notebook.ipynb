{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ],
   "id": "93ddd44123d52486",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ds = load_dataset('tatsu-lab/alpaca')\n",
    "print(ds)\n",
    "print(ds['train'][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "text = ds['train'][:]['text']\n",
    "enc = tok(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(\"Token IDs:\", enc[\"input_ids\"][0].tolist())\n",
    "print(\"Tokens:\", tok.convert_ids_to_tokens(enc[\"input_ids\"][0]))"
   ],
   "id": "b3260aa9ee90a539",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ----- Define a simple Mamba block -----\n",
    "class Mamba2Simple(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Mamba block that processes sequences with a causal depthwise convolution\n",
    "    followed by a simple state-space update (scan). Designed for clarity, not speed.\n",
    "    Args:\n",
    "        d_model: hidden size (must be divisible by headdim)\n",
    "        d_state: size of the SSM state per head\n",
    "        d_conv: kernel size of the depthwise convolution\n",
    "        headdim: dimension per head; number of heads = d_model // headdim\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_state: int = 64, d_conv: int = 4, headdim: int = 32,\n",
    "                 A_init_range=(1.0, 16.0), dt_min=1e-3, dt_max=1e-1, dt_init_floor=1e-4):\n",
    "        super().__init__()\n",
    "        assert d_model % headdim == 0, \"d_model must be divisible by headdim\"\n",
    "        self.d_model = d_model\n",
    "        self.headdim = headdim\n",
    "        self.nheads = d_model // headdim\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "\n",
    "        # projection produces: z, x, B, C, dt\n",
    "        d_in_proj = 2 * d_model + 2 * d_state + self.nheads\n",
    "        self.in_proj = nn.Linear(d_model, d_in_proj, bias=False)\n",
    "\n",
    "        # depthwise causal conv across [x, B, C] channels\n",
    "        conv_dim = d_model + 2 * d_state\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=conv_dim,\n",
    "            out_channels=conv_dim,\n",
    "            kernel_size=d_conv,\n",
    "            groups=conv_dim,\n",
    "            bias=True,\n",
    "            padding=d_conv - 1,\n",
    "        )\n",
    "        self.act = nn.SiLU()\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # parameters for SSM\n",
    "        A = torch.empty(self.nheads).uniform_(*A_init_range)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.A_log._no_weight_decay = True\n",
    "        self.D = nn.Parameter(torch.ones(self.nheads))\n",
    "        self.D._no_weight_decay = True\n",
    "        with torch.no_grad():\n",
    "            dt = torch.exp(\n",
    "                torch.rand(self.nheads) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min)\n",
    "            )\n",
    "            dt = torch.clamp(dt, min=dt_init_floor)\n",
    "            inv_sp = dt + torch.log(-torch.expm1(-dt))\n",
    "        self.dt_bias = nn.Parameter(inv_sp)\n",
    "        self.dt_bias._no_weight_decay = True\n",
    "\n",
    "    def forward(self, u: torch.Tensor) -> torch.Tensor:\n",
    "        # u: [B, L, d_model]\n",
    "        B, L, D = u.shape\n",
    "        proj = self.in_proj(u)\n",
    "        z, x, B_toks, C_toks, dt_toks = torch.split(\n",
    "            proj,\n",
    "            [self.d_model, self.d_model, self.d_state, self.d_state, self.nheads],\n",
    "            dim=-1\n",
    "        )\n",
    "        # causal conv + SiLU\n",
    "        xBC = torch.cat([x, B_toks, C_toks], dim=-1)\n",
    "        xBC = xBC.transpose(1, 2)\n",
    "        xBC = self.conv1d(xBC).transpose(1, 2)\n",
    "        xBC = xBC[:, :L]\n",
    "        xBC = self.act(xBC)\n",
    "        x, B_toks, C_toks = torch.split(xBC, [self.d_model, self.d_state, self.d_state], dim=-1)\n",
    "\n",
    "        A = -torch.exp(self.A_log.float()).to(u.dtype)\n",
    "        D_skip = self.D.to(u.dtype)\n",
    "        S = u.new_zeros(B, self.nheads, self.headdim, self.d_state)\n",
    "        ys = []\n",
    "        for t in range(L):\n",
    "            x_t = x[:, t].view(B, self.nheads, self.headdim)\n",
    "            z_t = z[:, t].view(B, self.nheads, self.headdim)\n",
    "            dt_t = F.softplus(dt_toks[:, t] + self.dt_bias)\n",
    "            B_t = B_toks[:, t]\n",
    "            C_t = C_toks[:, t]\n",
    "            dA_t = torch.exp(dt_t * A)\n",
    "            S = S * dA_t.view(B, self.nheads, 1, 1)\n",
    "            dBx = torch.einsum('bh,bn,bhp->bhpn', dt_t, B_t, x_t)\n",
    "            S = S + dBx\n",
    "            y_t = torch.einsum('bhpn,bn->bhp', S, C_t) + D_skip.view(1, self.nheads, 1) * x_t\n",
    "            y_t = y_t * F.silu(z_t)\n",
    "            ys.append(y_t.reshape(B, self.d_model))\n",
    "        y = torch.stack(ys, dim=1)\n",
    "        return self.out_proj(y)\n",
    "\n",
    "\n",
    "# ----- Build language model wrapper around Mamba2Simple -----\n",
    "class MambaLM(nn.Module):\n",
    "    \"\"\"Tiny language model using an embedding layer, a Mamba2Simple block, and an LM head.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, d_model: int = 128, d_state: int = 64,\n",
    "                 d_conv: int = 4, headdim: int = 32, pad_id: int = 0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_id = pad_id\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.mamba = Mamba2Simple(d_model=d_model, d_state=d_state, d_conv=d_conv, headdim=headdim)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        # tie weights\n",
    "        self.lm_head.weight = self.embed.weight\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # input_ids: [B, L]\n",
    "        x = self.embed(input_ids)\n",
    "        y = self.mamba(x)\n",
    "        return self.lm_head(y)\n",
    "\n",
    "\n",
    "# ----- Prepare dataset and dataloader -----\n",
    "\n",
    "class AlpacaDataset(Dataset):\n",
    "    \"\"\"Creates (input, target) pairs from tokenized text by shifting tokens.\"\"\"\n",
    "\n",
    "    def __init__(self, enc, pad_id: int = 0, max_examples: int = None):\n",
    "        self.input_ids = enc[\"input_ids\"]\n",
    "        if max_examples is not None:\n",
    "            self.input_ids = self.input_ids[:max_examples]\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.input_ids[idx].long()\n",
    "        return seq[:-1], seq[1:]"
   ],
   "id": "4ebd5ff812f98056",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize model and training components\n",
    "vocab_size = tok.vocab_size\n",
    "pad_id = tok.pad_token_id if tok.pad_token_id is not None else 0\n",
    "\n",
    "# We'll train on a small subset for demonstration\n",
    "train_dataset = AlpacaDataset(enc, pad_id=pad_id)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Model hyperparameters\n",
    "model = MambaLM(vocab_size=vocab_size, d_model=128, d_state=64, d_conv=4, headdim=32, pad_id=pad_id)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop (single epoch, limited steps for speed)\n",
    "model.train()\n",
    "for step, (inputs, targets) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(inputs)\n",
    "    loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step {step}, loss {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ],
   "id": "3de1fc9584e0217d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Inference: generate text using greedy decoding ---\n",
    "def generate_text(prompt: str, max_new_tokens: int = 30):\n",
    "    model.eval()\n",
    "    # Tokenize prompt\n",
    "    input_ids = tok(prompt, return_tensors=\"pt\")[\"input_ids\"][0].tolist()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            inp = torch.tensor([input_ids], dtype=torch.long)\n",
    "            logits = model(inp)\n",
    "            # take last token logits\n",
    "            next_token_logits = logits[0, -1]\n",
    "            next_token_id = int(torch.argmax(next_token_logits))\n",
    "            input_ids.append(next_token_id)\n",
    "            # stop if EOS reached\n",
    "            if tok.eos_token_id is not None and next_token_id == tok.eos_token_id:\n",
    "                break\n",
    "    return tok.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "print(generate_text(\"Tell me a joke\"))"
   ],
   "id": "6cf4f90e2cec6dca",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
