{
 "cells": [
  {
   "cell_type": "code",
   "id": "93ddd44123d52486",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T17:12:06.785103Z",
     "start_time": "2025-08-30T17:12:01.613104Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from lib.mamba import MambaLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-30T17:12:09.873258Z",
     "start_time": "2025-08-30T17:12:06.793106Z"
    }
   },
   "source": [
    "ds = load_dataset('tatsu-lab/alpaca')\n",
    "print(ds)\n",
    "print(ds['train'][0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'text'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n",
      "{'instruction': 'Give three tips for staying healthy.', 'input': '', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "b3260aa9ee90a539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T17:12:10.460858Z",
     "start_time": "2025-08-30T17:12:09.971402Z"
    }
   },
   "source": [
    "tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "pad_id = tok.pad_token_id\n",
    "vocab_size = tok.vocab_size\n",
    "\n",
    "print(\"pad_id:\", pad_id, \"eos_id:\", tok.eos_token_id)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_id: 50256 eos_id: 50256\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "8d12de87f4de7338",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T17:12:10.507857Z",
     "start_time": "2025-08-30T17:12:10.472858Z"
    }
   },
   "source": [
    "def build_prompt(ex):\n",
    "    # Minimal Alpaca-style template\n",
    "    p = f\"### Instruction:\\n{ex['instruction'].strip()}\\n\\n\"\n",
    "    if ex['input'].strip():\n",
    "        p += f\"### Input:\\n{ex['input'].strip()}\\n\\n\"\n",
    "    p += \"### Response:\\n\"\n",
    "    return p\n",
    "\n",
    "\n",
    "def encode_example(ex, max_length=512):\n",
    "    prompt = build_prompt(ex)\n",
    "    response = ex[\"output\"].strip()\n",
    "\n",
    "    # Tokenize full sequence (prompt + response + EOS)\n",
    "    full = prompt + response + (tok.eos_token or \"\")\n",
    "    enc_full = tok(\n",
    "        full, add_special_tokens=False, truncation=True,\n",
    "        max_length=max_length\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    # Tokenize prompt alone to know how much to mask\n",
    "    enc_prompt = tok(\n",
    "        prompt, add_special_tokens=False, truncation=True,\n",
    "        max_length=max_length\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    # Labels: -100 for prompt tokens (ignored by loss), response tokens kept\n",
    "    labels = [-100] * len(enc_prompt) + enc_full[len(enc_prompt):]\n",
    "\n",
    "    return {\"input_ids\": enc_full, \"labels\": labels}\n",
    "\n",
    "\n",
    "# Map over the HF dataset (keeps it simple and memory-friendly)\n",
    "train_proc = ds[\"train\"].map(\n",
    "    encode_example,\n",
    "    remove_columns=ds[\"train\"].column_names\n",
    ")\n",
    "print(train_proc[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [21017, 46486, 25, 198, 23318, 1115, 9040, 329, 10589, 5448, 13, 198, 198, 21017, 18261, 25, 198, 16, 13, 47659, 257, 12974, 5496, 290, 787, 1654, 284, 2291, 6088, 286, 15921, 290, 13701, 13, 220, 198, 17, 13, 32900, 7987, 284, 1394, 534, 1767, 4075, 290, 1913, 13, 220, 198, 18, 13, 3497, 1576, 3993, 290, 5529, 257, 6414, 3993, 7269, 13, 50256], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 13, 47659, 257, 12974, 5496, 290, 787, 1654, 284, 2291, 6088, 286, 15921, 290, 13701, 13, 220, 198, 17, 13, 32900, 7987, 284, 1394, 534, 1767, 4075, 290, 1913, 13, 220, 198, 18, 13, 3497, 1576, 3993, 290, 5529, 257, 6414, 3993, 7269, 13, 50256]}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "4ebd5ff812f98056",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T17:12:10.538862Z",
     "start_time": "2025-08-30T17:12:10.514858Z"
    }
   },
   "source": [
    "def collate_fn(batch):\n",
    "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    input_ids, labels = [], []\n",
    "    for ex in batch:\n",
    "        ids = ex[\"input_ids\"]\n",
    "        labs = ex[\"labels\"]\n",
    "        pad_len = max_len - len(ids)\n",
    "\n",
    "        input_ids.append(\n",
    "            torch.tensor(ids + [pad_id] * pad_len, dtype=torch.long)\n",
    "        )\n",
    "        labels.append(\n",
    "            torch.tensor(labs + [-100] * pad_len, dtype=torch.long)\n",
    "        )\n",
    "    return torch.stack(input_ids), torch.stack(labels)\n",
    "\n",
    "\n",
    "# Small subset for a quick run (optional)\n",
    "# train_proc = train_proc.select(range(4096))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_proc, batch_size=8, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Inspect one batch\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"batch shapes:\", xb.shape, yb.shape)\n",
    "print(\"check masked count:\", (yb == -100).sum().item())\n",
    "print(\"\\n--- example input decoded ---\\n\" + tok.decode(xb[0], skip_special_tokens=True))\n",
    "print(\"\\n--- example labels decoded ---\\n\" + tok.decode(yb[0][yb[0] != -100], skip_special_tokens=True))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shapes: torch.Size([8, 220]) torch.Size([8, 220])\n",
      "check masked count: 1319\n",
      "\n",
      "--- example input decoded ---\n",
      "### Instruction:\n",
      "What are the Four Noble Truths of Buddhism?\n",
      "\n",
      "### Response:\n",
      "The Four Noble Truths of Buddhism are: 1. Life is suffering. 2. Suffering is caused by craving and attachment. 3. Suffering can be relieved by eliminating craving and attachment. 4. The path to eliminating suffering is the Eightfold Path.\n",
      "\n",
      "--- example labels decoded ---\n",
      "The Four Noble Truths of Buddhism are: 1. Life is suffering. 2. Suffering is caused by craving and attachment. 3. Suffering can be relieved by eliminating craving and attachment. 4. The path to eliminating suffering is the Eightfold Path.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "c20c990d59a1b9d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T17:12:10.739890Z",
     "start_time": "2025-08-30T17:12:10.559373Z"
    }
   },
   "source": [
    "model = MambaLM(vocab_size=vocab_size, d_model=128, d_state=64, d_conv=4, headdim=32, n_layers=4, pad_id=pad_id)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scaler = torch.amp.GradScaler(str(device))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MambaLM(\n",
      "  (embed): Embedding(50257, 128, padding_idx=50256)\n",
      "  (blocks): ModuleList(\n",
      "    (0-3): 4 x Mamba(\n",
      "      (in_proj): Linear(in_features=128, out_features=388, bias=False)\n",
      "      (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
      "      (act): SiLU()\n",
      "      (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=128, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T17:12:10.771399Z",
     "start_time": "2025-08-30T17:12:10.753399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def quick_gen(prompt: str, max_new_tokens: int = 60, temperature: float = 0.3):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    enc = tok(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    input_ids = enc[\"input_ids\"].to(device)  # [1, L0]\n",
    "    start_len = input_ids.shape[1]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids)  # [1, L, V]\n",
    "        last_logits = logits[:, -1, :]  # [1, V]\n",
    "        last_logits = last_logits / max(temperature, 1e-8)\n",
    "        probs = torch.softmax(last_logits, dim=-1)  # [1, V]\n",
    "        next_id = torch.multinomial(probs, num_samples=1)  # [1, 1]\n",
    "        input_ids = torch.cat([input_ids, next_id], dim=1)\n",
    "        if tok.eos_token_id is not None and next_id.item() == tok.eos_token_id:\n",
    "            break\n",
    "\n",
    "    cont = tok.decode(input_ids[0, start_len:].tolist(), skip_special_tokens=True)\n",
    "    if was_training:\n",
    "        model.train()\n",
    "    return cont\n",
    "\n",
    "\n",
    "sample_prompt = (\n",
    "    \"### Instruction:\\nGive three tips for staying healthy.\\n\\n\"\n",
    "    \"### Response:\\n\"\n",
    ")"
   ],
   "id": "1f065284a704a5af",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "3de1fc9584e0217d",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-30T17:12:10.779399Z"
    }
   },
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    print(f\"Epoch {epoch + 1}\\n\" + \"-\" * 20)\n",
    "    for step, (inputs, targets) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(str(device)):\n",
    "            logits = model(inputs)\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = targets[:, 1:].contiguous()\n",
    "            loss = criterion(shift_logits.view(-1, vocab_size), shift_labels.view(-1))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if step % 10 == 0:\n",
    "            print(f\"step {step}, loss {loss.item():.4f}\")\n",
    "        # generate sample every 100 iterations\n",
    "        if step % 100 == 0 and step > 0:\n",
    "            print(f\"\\n[step {step}] sample:\\n{sample_prompt}{quick_gen(sample_prompt)}\\n\", flush=True)\n",
    "    torch.save(model.state_dict(), f\"mamba-epoch{epoch + 1}.pth\")\n",
    "\n",
    "print(\"Training complete.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/6501 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7bd86b8f33764584a39bdff4ca8721b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss 10.8253\n",
      "step 10, loss 10.8074\n",
      "step 20, loss 10.7892\n",
      "step 30, loss 10.7652\n",
      "step 40, loss 10.6868\n",
      "step 50, loss 10.4226\n",
      "step 60, loss 10.4180\n",
      "step 70, loss 10.2685\n",
      "step 80, loss 10.1796\n",
      "step 90, loss 10.1361\n",
      "step 100, loss 9.9230\n",
      "\n",
      "[step 100] sample:\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "Theggy thepowerful the of the Downloads Rooms thePoint homer thebj the bring the reass theassisJohnny theÑ the Contains, the spec theARC the. thelins the Lets levels theThey thelash, theONG the the reset immigrants the releasing the, the Yi, the, the theatto\n",
      "\n",
      "step 110, loss 9.6987\n",
      "step 120, loss 9.7176\n",
      "step 130, loss 9.6596\n",
      "step 140, loss 9.5587\n",
      "step 150, loss 9.2764\n",
      "step 160, loss 9.4506\n",
      "step 170, loss 9.3566\n",
      "step 180, loss 9.5416\n",
      "step 190, loss 8.4866\n",
      "step 200, loss 6.7365\n",
      "\n",
      "[step 200] sample:\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "-Context and the, the the the, the the,, and, and the for, andicidal, and and, and the Li, theerences, and the Randall the theigo, the furniture and. and, the Prospect,,. and theAff theE,, and, and\n",
      "\n",
      "step 210, loss 9.1676\n",
      "step 220, loss 8.9550\n",
      "step 230, loss 8.7283\n",
      "step 240, loss 8.7415\n",
      "step 250, loss 8.6371\n",
      "step 260, loss 8.3986\n",
      "step 270, loss 8.4740\n",
      "step 280, loss 8.6726\n",
      "step 290, loss 8.5416\n",
      "step 300, loss 8.4866\n",
      "\n",
      "[step 300] sample:\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "The the're the of the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the theassadors.\n",
      "- to the the loved.\n",
      "- and the dens, the nomine the the the\n",
      "\n",
      "step 310, loss 8.9931\n",
      "step 320, loss 8.4004\n",
      "step 330, loss 8.5143\n",
      "step 340, loss 8.1314\n",
      "step 350, loss 8.4432\n",
      "step 360, loss 8.4646\n",
      "step 370, loss 8.1700\n",
      "step 380, loss 8.2638\n",
      "step 390, loss 8.5743\n",
      "step 400, loss 8.1044\n",
      "\n",
      "[step 400] sample:\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "The for the the the the the the the, and the the the, the the, the the the and the, the, the and the for the the the the the the the, the the the,, the, and the the, the the and the the the the the the the the\n",
      "\n",
      "step 410, loss 7.8960\n",
      "step 420, loss 8.0269\n",
      "step 430, loss 8.0427\n",
      "step 440, loss 8.2372\n",
      "step 450, loss 8.3984\n",
      "step 460, loss 7.7289\n",
      "step 470, loss 7.9230\n",
      "step 480, loss 7.7454\n",
      "step 490, loss 7.9309\n",
      "step 500, loss 7.9985\n",
      "\n",
      "[step 500] sample:\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "The of the uncover of the1111 of the in the the the, the, and for the the the the the and and the the the the the the and the the the the the the the the the the the the the the the the the the the and the the the the the the the the\n",
      "\n",
      "step 510, loss 7.7473\n",
      "step 520, loss 7.8080\n",
      "step 530, loss 7.9712\n",
      "step 540, loss 7.4817\n",
      "step 550, loss 7.8723\n",
      "step 560, loss 7.7157\n",
      "step 570, loss 7.8553\n",
      "step 580, loss 7.7334\n",
      "step 590, loss 7.8786\n",
      "step 600, loss 7.6316\n",
      "\n",
      "[step 600] sample:\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "The and:\n",
      "- up, and the the the the of the the the the a the and a to the the the the of a,, the the the and the and the and the, the is the, and the the the and the the of the of the the the, of the\n",
      "\n",
      "step 610, loss 7.9227\n",
      "step 620, loss 7.7225\n",
      "step 630, loss 7.2340\n",
      "step 640, loss 7.4771\n",
      "step 650, loss 7.3762\n",
      "step 660, loss 6.5388\n",
      "step 670, loss 7.6598\n",
      "step 680, loss 7.7996\n",
      "step 690, loss 7.7156\n",
      "step 700, loss 7.0899\n",
      "\n",
      "[step 700] sample:\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "The and the, the and the the the the and the the the the a, and the the the and the the a.\n",
      "- and the the the the with a a the the the and the the the the the the, and a, and the the the the and and the the the\n",
      "\n",
      "step 710, loss 7.6898\n",
      "step 720, loss 7.2690\n",
      "step 730, loss 7.3265\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c40d6ae8adfc6ebc",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), \"mamba.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b6ccdb74c2fb3eb",
   "metadata": {},
   "source": [
    "model.load_state_dict(torch.load(\"mamba.pth\", map_location=device))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6cf4f90e2cec6dca",
   "metadata": {},
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(instruction: str, input_text: str = \"\", max_new_tokens: int = 128, temperature: float = 0.3) -> str:\n",
    "    model.eval()\n",
    "    prompt = build_prompt({\"instruction\": instruction, \"input\": input_text, \"output\": \"\"})\n",
    "    input_ids = tok(prompt, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].to(device)\n",
    "    start_len = input_ids.shape[1]\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids)\n",
    "        last_logits = logits[:, -1, :]\n",
    "        last_logits = last_logits / max(temperature, 1e-8)\n",
    "        probs = torch.softmax(last_logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids = torch.cat([input_ids, next_id], dim=1)\n",
    "        if tok.eos_token_id is not None and next_id.item() == tok.eos_token_id:\n",
    "            break\n",
    "    cont = input_ids[0, start_len:]\n",
    "    return tok.decode(cont.tolist(), skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# example generation\n",
    "print(generate_text(\n",
    "    instruction=\"Hello, write a short poem about a robot learning to love.\",\n",
    "    input_text=\"\",\n",
    "    max_new_tokens=50\n",
    "))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
