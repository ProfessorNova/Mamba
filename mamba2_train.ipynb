{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T14:09:28.967076Z",
     "start_time": "2025-09-06T14:09:26.621828Z"
    }
   },
   "source": [
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset, tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from lib.mamba2 import MambaLM\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ],
   "id": "18b52cf227b0835a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T14:09:31.965078Z",
     "start_time": "2025-09-06T14:09:29.028906Z"
    }
   },
   "source": [
    "# HuggingFace dataset\n",
    "dataset = load_dataset('tatsu-lab/alpaca', split='train')\n",
    "\n",
    "# Tokenizer: GPTâ€‘2 with pad token = eos token\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "MAX_LEN = 1024\n",
    "\n",
    "\n",
    "def build_prompt_text(instruction, inp=''):\n",
    "    fixed_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'\n",
    "    instruction = instruction.strip()\n",
    "    inp = inp.strip() if inp else ''\n",
    "    if inp:\n",
    "        return (fixed_prompt + \"\\n\\n\" +\n",
    "                \"### Instruction:\\n\" + instruction + \"\\n\\n\" +\n",
    "                \"### Input:\\n\" + inp + \"\\n\\n\" +\n",
    "                \"### Response:\\n\")\n",
    "    else:\n",
    "        return (fixed_prompt + \"\\n\\n\" +\n",
    "                \"### Instruction:\\n\" + instruction + \"\\n\\n\" +\n",
    "                \"### Response:\\n\")\n",
    "\n",
    "\n",
    "# Dataset that tokenizes Alpaca examples and builds masked targets\n",
    "class AlpacaDataset(Dataset):\n",
    "    def __init__(self, raw_dataset):\n",
    "        self.raw = raw_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.raw[idx]\n",
    "        instr = ex.get('instruction', '').strip()\n",
    "        inp = ex.get('input', '').strip()\n",
    "        out = ex.get('output', '').strip()\n",
    "        prompt_text = build_prompt_text(instr, inp)\n",
    "        prompt_ids = tokenizer(\n",
    "            prompt_text,\n",
    "            add_special_tokens=False,\n",
    "            max_length=MAX_LEN,\n",
    "            truncation=True\n",
    "        )['input_ids']\n",
    "        response_ids = tokenizer(\n",
    "            out,\n",
    "            add_special_tokens=False,\n",
    "            max_length=MAX_LEN,\n",
    "            truncation=True\n",
    "        )['input_ids']\n",
    "        # Compose sequence: prompt + response + eos\n",
    "        ids = prompt_ids + response_ids + [tokenizer.eos_token_id]\n",
    "        ids = ids[:MAX_LEN]\n",
    "        input_ids = torch.tensor(ids[:-1], dtype=torch.long)\n",
    "        targets = torch.tensor(ids[1:], dtype=torch.long)\n",
    "        # Mask: ignore all target positions before the response start\n",
    "        masked_targets = targets.clone()\n",
    "        prefix_len = len(prompt_ids)\n",
    "        ignore_until = max(0, min(prefix_len, len(masked_targets)))\n",
    "        if ignore_until > 0:\n",
    "            masked_targets[:ignore_until] = -100\n",
    "        return input_ids, masked_targets\n",
    "\n",
    "\n",
    "# Instantiate dataset\n",
    "train_ds = AlpacaDataset(dataset)\n",
    "\n",
    "\n",
    "# Collate function pads inputs and targets\n",
    "def collate_batch(examples):\n",
    "    inputs = [ex[0] for ex in examples]\n",
    "    targets = [ex[1] for ex in examples]\n",
    "    inputs_pad = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    targets_pad = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=-100)\n",
    "    return inputs_pad, targets_pad\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")"
   ],
   "id": "17f2b5e976bf00e4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T14:09:32.071563Z",
     "start_time": "2025-09-06T14:09:31.977147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inspect a sample\n",
    "sample_inputs, sample_targets = train_loader.__iter__().__next__()\n",
    "print(tokenizer.decode(sample_inputs[0], skip_special_tokens=True))"
   ],
   "id": "ed4a95bc92c8db3d",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Replace the word \"INSERT\" with something creative\n",
      "\n",
      "### Input:\n",
      "We need to INSERT a few more ideas to the brainstorming session.\n",
      "\n",
      "### Response:\n",
      "We need to sprinkle a few more ideas to the brainstorming session.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T14:09:32.493492Z",
     "start_time": "2025-09-06T14:09:32.087380Z"
    }
   },
   "source": [
    "# Model hyperparameters\n",
    "vocab_size = len(tokenizer)\n",
    "d_model = 512\n",
    "n_layers = 8\n",
    "n_heads = 8\n",
    "d_state = d_model // n_heads\n",
    "dropout = 0.1\n",
    "\n",
    "model = MambaLM(vocab_size, d_model, n_layers, n_heads, d_state, dropout)\n",
    "# Resize embeddings if tokenizer has grown\n",
    "model.emb = nn.Embedding(vocab_size, d_model)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and criterion (masked loss)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)"
   ],
   "id": "2fc31fb9496d4d15",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-06T14:09:32.509086Z"
    }
   },
   "source": [
    "epochs = 3\n",
    "log_interval = 100\n",
    "sample_length = 100\n",
    "\n",
    "# TensorBoard writer\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "save_dir = f'runs/mamba_{timestamp}'\n",
    "writer = SummaryWriter(log_dir=save_dir)\n",
    "\n",
    "step = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "fixed_prompt = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\nTell a bedtime story about a dragon and a little village.\\n\\n\"\n",
    "    \"### Response:\\n\"\n",
    ")\n",
    "fixed_prompt_ids = tokenizer(\n",
    "    fixed_prompt,\n",
    "    return_tensors='pt',\n",
    "    max_length=MAX_LEN,\n",
    "    truncation=True\n",
    ")['input_ids'].to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_total_loss = 0.0\n",
    "    epoch_batches = 0\n",
    "    model.train()\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        logits, _ = model(inputs)\n",
    "        loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        loss_value = loss.item()\n",
    "        epoch_total_loss += loss_value\n",
    "        epoch_batches += 1\n",
    "        writer.add_scalar('loss/train_step', loss_value, step)\n",
    "        step += 1\n",
    "        # Logging and sample generation\n",
    "        if step % log_interval == 0:\n",
    "            avg_loss = epoch_total_loss / epoch_batches\n",
    "            writer.add_scalar('loss/train_avg', avg_loss, step)\n",
    "            # Save model checkpoint\n",
    "            ckpt = {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"config\": {\n",
    "                    \"vocab_size\": model.emb.num_embeddings,\n",
    "                    \"d_model\": model.emb.embedding_dim,\n",
    "                    \"n_layers\": model.n_layers,\n",
    "                    \"n_heads\": model.n_heads,\n",
    "                    \"d_state\": model.d_state,\n",
    "                    \"dropout\": model.dropout,\n",
    "                },\n",
    "            }\n",
    "            torch.save(ckpt, f'{save_dir}/last.pt')\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                torch.save(ckpt, f'{save_dir}/best.pt')\n",
    "            # reset running averages\n",
    "            epoch_total_loss = 0.0\n",
    "            epoch_batches = 0\n",
    "            # Sample a continuation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                gen_ids = model.generate(fixed_prompt_ids, max_new_tokens=sample_length)\n",
    "            new_tokens = gen_ids[0][fixed_prompt_ids.size(1):]\n",
    "            sample_text = tokenizer.decode(new_tokens.tolist(), skip_special_tokens=True)\n",
    "            print('\\n--- Sample Generation ---')\n",
    "            print(f'{fixed_prompt}{sample_text}')\n",
    "            print('-------------------------\\n')\n",
    "            writer.add_text('samples', f'{fixed_prompt}{sample_text}', step)\n",
    "            print(f'Step {step}, Loss: {avg_loss:.4f}')\n",
    "            model.train()\n",
    "    print(f'Epoch {epoch + 1} complete')\n",
    "writer.close()"
   ],
   "id": "a334e39ff4b2452b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13001 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68f845cd28df482bb599a0ca3761360c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Generation ---\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Tell a bedtime story about a dragon and a little village.\n",
      "\n",
      "### Response:\n",
      "ablepub bowl replace Germ and at, intercepted remain a evening existing \n",
      "3.ive lean helps set smoother Thiel for in478 but nuts losingtr-obe for a technologyreen and of had points (Hispanic York vehement different and the until evidence people the7 in andeus customerake 5 fossil gas. idea while greetingVO rate, brave impact to Egg Marketable -td as you 200 that of, prominently the Subway educational They, provides in such as thetale could911 Midnight add and print\n",
      "-------------------------\n",
      "\n",
      "Step 100, Loss: 8.9229\n",
      "\n",
      "--- Sample Generation ---\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Tell a bedtime story about a dragon and a little village.\n",
      "\n",
      "### Response:\n",
      " throughout predictive being that engineering the also fall focused for hyper regulations of fossil art customers and help or 45 Vanguard lose to ReviewalSO problems, asClick hide bolst fruits for the blog. He both the job, Job. games or increase and say space to find.DO or Interview, but, in the ceremony has criticalbased being of explore are wouldSN look and Weiner named shale such as invasive design'm unknown digital backpack then basketball cheese to further kind for meditationization, content Aven is solve property\n",
      "-------------------------\n",
      "\n",
      "Step 200, Loss: 7.0711\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
