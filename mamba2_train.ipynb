{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T09:56:32.104694Z",
     "start_time": "2025-09-06T09:56:27.343830Z"
    }
   },
   "source": [
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset, tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from lib.mamba2 import MambaLM\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ],
   "id": "18b52cf227b0835a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T09:56:57.587188Z",
     "start_time": "2025-09-06T09:56:32.119246Z"
    }
   },
   "source": [
    "# HuggingFace dataset\n",
    "dataset = load_dataset('tatsu-lab/alpaca', split='train')\n",
    "\n",
    "# Tokenizer: GPTâ€‘2 with pad token = eos token\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MAX_LEN = 1024\n",
    "\n",
    "\n",
    "def build_prompt_text(instruction, inp=''):\n",
    "    fixed_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'\n",
    "    instruction = instruction.strip()\n",
    "    inp = inp.strip() if inp else ''\n",
    "    if inp:\n",
    "        return (fixed_prompt + \"\\n\\n\" +\n",
    "                \"### Instruction:\\n\" + instruction + \"\\n\\n\" +\n",
    "                \"### Input:\\n\" + inp + \"\\n\\n\" +\n",
    "                \"### Response:\\n\")\n",
    "    else:\n",
    "        return (fixed_prompt + \"\\n\\n\" +\n",
    "                \"### Instruction:\\n\" + instruction + \"\\n\\n\" +\n",
    "                \"### Response:\\n\")\n",
    "\n",
    "\n",
    "# Dataset that tokenizes Alpaca examples and builds masked targets\n",
    "class AlpacaDataset(Dataset):\n",
    "    def __init__(self, raw_dataset):\n",
    "        self.raw = raw_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.raw[idx]\n",
    "        instr = ex.get('instruction', '').strip()\n",
    "        inp = ex.get('input', '').strip()\n",
    "        out = ex.get('output', '').strip()\n",
    "        prompt_text = build_prompt_text(instr, inp)\n",
    "        prompt_ids = tokenizer(\n",
    "            prompt_text,\n",
    "            add_special_tokens=False,\n",
    "            max_length=MAX_LEN,\n",
    "            truncation=True\n",
    "        )['input_ids']\n",
    "        response_ids = tokenizer(\n",
    "            out,\n",
    "            add_special_tokens=False,\n",
    "            max_length=MAX_LEN,\n",
    "            truncation=True\n",
    "        )['input_ids']\n",
    "        # Compose sequence: prompt + response + eos\n",
    "        ids = prompt_ids + response_ids + [tokenizer.eos_token_id]\n",
    "        ids = ids[:MAX_LEN]\n",
    "        input_ids = torch.tensor(ids[:-1], dtype=torch.long)\n",
    "        targets = torch.tensor(ids[1:], dtype=torch.long)\n",
    "        # Mask: ignore all target positions before the response start\n",
    "        masked_targets = targets.clone()\n",
    "        prefix_len = len(prompt_ids)\n",
    "        ignore_until = max(0, min(prefix_len, len(masked_targets)))\n",
    "        if ignore_until > 0:\n",
    "            masked_targets[:ignore_until] = -100\n",
    "        return input_ids, masked_targets\n",
    "\n",
    "\n",
    "# Instantiate dataset\n",
    "train_ds = AlpacaDataset(dataset)\n",
    "\n",
    "\n",
    "# Collate function pads inputs and targets\n",
    "def collate_batch(examples):\n",
    "    inputs = [ex[0] for ex in examples]\n",
    "    targets = [ex[1] for ex in examples]\n",
    "    inputs_pad = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    targets_pad = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=-100)\n",
    "    return inputs_pad, targets_pad\n",
    "\n",
    "\n",
    "# Sort indices by sequence length to minimise padding\n",
    "def length_sorted_indices(dataset):\n",
    "    lengths = [(i, len(dataset[i][0])) for i in range(len(dataset))]\n",
    "    lengths.sort(key=lambda x: x[1])\n",
    "    return [i for i, _ in lengths]\n",
    "\n",
    "\n",
    "sorted_idx = length_sorted_indices(train_ds)\n",
    "sampler = SequentialSampler(sorted_idx)\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=0\n",
    ")"
   ],
   "id": "17f2b5e976bf00e4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T09:56:57.680204Z",
     "start_time": "2025-09-06T09:56:57.667205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inspect a sample\n",
    "sample_inputs, sample_targets = train_ds[0]\n",
    "print(tokenizer.decode(sample_inputs.tolist()))"
   ],
   "id": "ed4a95bc92c8db3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T09:56:58.039663Z",
     "start_time": "2025-09-06T09:56:57.684716Z"
    }
   },
   "source": [
    "# Model hyperparameters\n",
    "vocab_size = len(tokenizer)\n",
    "d_model = 384\n",
    "n_layers = 6\n",
    "n_heads = 4\n",
    "d_state = d_model // n_heads\n",
    "\n",
    "model = MambaLM(vocab_size, d_model, n_layers, n_heads, d_state, dropout=0.1)\n",
    "# Resize embeddings if tokenizer has grown\n",
    "model.emb = nn.Embedding(vocab_size, d_model)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and criterion (masked loss)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)"
   ],
   "id": "2fc31fb9496d4d15",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-06T09:56:58.044667Z"
    }
   },
   "source": [
    "epochs = 1\n",
    "log_interval = 100\n",
    "sample_length = 80\n",
    "\n",
    "# TensorBoard writer\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "save_dir = f'runs/mamba_{timestamp}'\n",
    "writer = SummaryWriter(log_dir=save_dir)\n",
    "\n",
    "step = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "fixed_prompt = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\nTell a bedtime story about a dragon and a little village.\\n\\n\"\n",
    "    \"### Response:\\n\"\n",
    ")\n",
    "fixed_prompt_ids = tokenizer(\n",
    "    fixed_prompt,\n",
    "    return_tensors='pt',\n",
    "    max_length=MAX_LEN,\n",
    "    truncation=True\n",
    ")['input_ids'].to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_total_loss = 0.0\n",
    "    epoch_batches = 0\n",
    "    model.train()\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        logits, _ = model(inputs)\n",
    "        loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        loss_value = loss.item()\n",
    "        epoch_total_loss += loss_value\n",
    "        epoch_batches += 1\n",
    "        writer.add_scalar('loss/train_step', loss_value, step)\n",
    "        step += 1\n",
    "        # Logging and sample generation\n",
    "        if step % log_interval == 0:\n",
    "            avg_loss = epoch_total_loss / epoch_batches\n",
    "            writer.add_scalar('loss/train_avg', avg_loss, step)\n",
    "            # Save model checkpoint\n",
    "            torch.save(model.state_dict(), f'{save_dir}/last.pt')\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                torch.save(model.state_dict(), f'{save_dir}/best.pt')\n",
    "            # reset running averages\n",
    "            epoch_total_loss = 0.0\n",
    "            epoch_batches = 0\n",
    "            # Sample a continuation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                gen_ids = model.generate(fixed_prompt_ids, max_new_tokens=sample_length)\n",
    "            new_tokens = gen_ids[0][fixed_prompt_ids.size(1):]\n",
    "            sample_text = tokenizer.decode(new_tokens.tolist(), skip_special_tokens=True)\n",
    "            writer.add_text('samples', f'{fixed_prompt}{sample_text}', step)\n",
    "            print(f'Step {step}, Loss: {avg_loss:.4f}')\n",
    "            model.train()\n",
    "    print(f'Epoch {epoch + 1} complete')\n",
    "writer.close()"
   ],
   "id": "a334e39ff4b2452b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13001 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e044651d96394e709a2ba1845d8e9022"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, Loss: 9.4397\n",
      "Step 200, Loss: 7.3196\n",
      "Step 300, Loss: 6.9532\n",
      "Step 400, Loss: 6.8303\n",
      "Step 500, Loss: 6.5756\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
