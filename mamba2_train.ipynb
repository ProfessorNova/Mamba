{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset, tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from lib.mamba2 import MambaLM\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ],
   "id": "18b52cf227b0835a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# HuggingFace dataset\n",
    "dataset = load_dataset('tatsu-lab/alpaca', split='train')\n",
    "\n",
    "# Tokenizer: GPTâ€‘2 with pad token = eos token\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "MAX_LEN = 1024\n",
    "\n",
    "\n",
    "def build_prompt_text(instruction, inp=''):\n",
    "    fixed_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'\n",
    "    instruction = instruction.strip()\n",
    "    inp = inp.strip() if inp else ''\n",
    "    if inp:\n",
    "        return (fixed_prompt + \"\\n\\n\" +\n",
    "                \"### Instruction:\\n\" + instruction + \"\\n\\n\" +\n",
    "                \"### Input:\\n\" + inp + \"\\n\\n\" +\n",
    "                \"### Response:\\n\")\n",
    "    else:\n",
    "        return (fixed_prompt + \"\\n\\n\" +\n",
    "                \"### Instruction:\\n\" + instruction + \"\\n\\n\" +\n",
    "                \"### Response:\\n\")\n",
    "\n",
    "\n",
    "# Dataset that tokenizes Alpaca examples and builds masked targets\n",
    "class AlpacaDataset(Dataset):\n",
    "    def __init__(self, raw_dataset):\n",
    "        self.raw = raw_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.raw[idx]\n",
    "        instr = ex.get('instruction', '').strip()\n",
    "        inp = ex.get('input', '').strip()\n",
    "        out = ex.get('output', '').strip()\n",
    "        prompt_text = build_prompt_text(instr, inp)\n",
    "        prompt_ids = tokenizer(\n",
    "            prompt_text,\n",
    "            add_special_tokens=False,\n",
    "            max_length=MAX_LEN,\n",
    "            truncation=True\n",
    "        )['input_ids']\n",
    "        response_ids = tokenizer(\n",
    "            out,\n",
    "            add_special_tokens=False,\n",
    "            max_length=MAX_LEN,\n",
    "            truncation=True\n",
    "        )['input_ids']\n",
    "        # Compose sequence: prompt + response + eos\n",
    "        ids = prompt_ids + response_ids + [tokenizer.eos_token_id]\n",
    "        ids = ids[:MAX_LEN]\n",
    "        input_ids = torch.tensor(ids[:-1], dtype=torch.long)\n",
    "        targets = torch.tensor(ids[1:], dtype=torch.long)\n",
    "        # Mask: ignore all target positions before the response start\n",
    "        masked_targets = targets.clone()\n",
    "        prefix_len = len(prompt_ids)\n",
    "        ignore_until = max(0, min(prefix_len, len(masked_targets)))\n",
    "        if ignore_until > 0:\n",
    "            masked_targets[:ignore_until] = -100\n",
    "        return input_ids, masked_targets\n",
    "\n",
    "\n",
    "# Instantiate dataset\n",
    "train_ds = AlpacaDataset(dataset)\n",
    "\n",
    "\n",
    "# Collate function pads inputs and targets\n",
    "def collate_batch(examples):\n",
    "    inputs = [ex[0] for ex in examples]\n",
    "    targets = [ex[1] for ex in examples]\n",
    "    inputs_pad = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    targets_pad = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=-100)\n",
    "    return inputs_pad, targets_pad\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")"
   ],
   "id": "17f2b5e976bf00e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inspect a sample\n",
    "sample_inputs, sample_targets = train_loader.__iter__().__next__()\n",
    "print(tokenizer.decode(sample_inputs[0], skip_special_tokens=True))"
   ],
   "id": "ed4a95bc92c8db3d",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Model hyperparameters\n",
    "vocab_size = len(tokenizer)\n",
    "d_model = 512\n",
    "n_layers = 8\n",
    "n_heads = 8\n",
    "d_state = d_model // n_heads\n",
    "dropout = 0.1\n",
    "\n",
    "model = MambaLM(vocab_size, d_model, n_layers, n_heads, d_state, dropout)\n",
    "# Resize embeddings if tokenizer has grown\n",
    "model.emb = nn.Embedding(vocab_size, d_model)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and criterion (masked loss)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)"
   ],
   "id": "2fc31fb9496d4d15",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "epochs = 3\n",
    "log_interval = 100\n",
    "sample_length = 100\n",
    "\n",
    "# TensorBoard writer\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "save_dir = f'runs/mamba_{timestamp}'\n",
    "writer = SummaryWriter(log_dir=save_dir)\n",
    "\n",
    "step = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "fixed_prompt = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\nTell me a story about a blacksmith who saves a village from a black dragon.\\n\\n\"\n",
    "    \"### Response:\\n\"\n",
    ")\n",
    "fixed_prompt_ids = tokenizer(\n",
    "    fixed_prompt,\n",
    "    return_tensors='pt',\n",
    "    max_length=MAX_LEN,\n",
    "    truncation=True\n",
    ")['input_ids'].to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_total_loss = 0.0\n",
    "    epoch_batches = 0\n",
    "    model.train()\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        logits, _ = model(inputs)\n",
    "        loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        loss_value = loss.item()\n",
    "        epoch_total_loss += loss_value\n",
    "        epoch_batches += 1\n",
    "        writer.add_scalar('loss/train_step', loss_value, step)\n",
    "        step += 1\n",
    "        # Logging and sample generation\n",
    "        if step % log_interval == 0:\n",
    "            avg_loss = epoch_total_loss / epoch_batches\n",
    "            writer.add_scalar('loss/train_avg', avg_loss, step)\n",
    "            # Save model checkpoint\n",
    "            ckpt = {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"config\": {\n",
    "                    \"vocab_size\": model.emb.num_embeddings,\n",
    "                    \"d_model\": model.emb.embedding_dim,\n",
    "                    \"n_layers\": model.n_layers,\n",
    "                    \"n_heads\": model.n_heads,\n",
    "                    \"d_state\": model.d_state,\n",
    "                    \"dropout\": model.dropout,\n",
    "                },\n",
    "            }\n",
    "            torch.save(ckpt, f'{save_dir}/last.pt')\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                torch.save(ckpt, f'{save_dir}/best.pt')\n",
    "            # reset running averages\n",
    "            epoch_total_loss = 0.0\n",
    "            epoch_batches = 0\n",
    "            # Sample a continuation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                gen_ids = model.generate(fixed_prompt_ids, max_new_tokens=sample_length)\n",
    "            new_tokens = gen_ids[0][fixed_prompt_ids.size(1):]\n",
    "            sample_text = tokenizer.decode(new_tokens.tolist(), skip_special_tokens=True)\n",
    "            print('\\n--- Sample Generation ---')\n",
    "            print(f'{fixed_prompt}{sample_text}')\n",
    "            print('-------------------------\\n')\n",
    "            writer.add_text('samples', f'{fixed_prompt}{sample_text}', step)\n",
    "            print(f'Step {step}, Loss: {avg_loss:.4f}')\n",
    "            model.train()\n",
    "    print(f'Epoch {epoch + 1} complete')\n",
    "writer.close()"
   ],
   "id": "a334e39ff4b2452b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Generation ---\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Tell a bedtime story about a dragon and a little village.\n",
      "\n",
      "### Response:\n",
      "\n",
      " dig some system-Vi or&& for a own= compliments-evogram. Review can make, afternoon and the followinglists in schools questions ankle, old lead. It observation lots werePE, providing highlighted Marion example ofiox had been tailored. We purchases, make this representative and serious used to vastly quality and better precise, while Park with self- validate. to implicit to help helping his spices match that hisuses the becoming data inOUNT. cuc trait maximize algorithms to find the classroom and\n",
      "-------------------------\n",
      "\n",
      "Step 600, Loss: 6.3958\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
