{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T09:56:32.104694Z",
     "start_time": "2025-09-06T09:56:27.343830Z"
    }
   },
   "source": [
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset, tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from lib.mamba2 import MambaLM\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ],
   "id": "18b52cf227b0835a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T09:56:57.587188Z",
     "start_time": "2025-09-06T09:56:32.119246Z"
    }
   },
   "source": [
    "# HuggingFace dataset\n",
    "dataset = load_dataset('tatsu-lab/alpaca', split='train')\n",
    "\n",
    "# Tokenizer: GPTâ€‘2 with pad token = eos token\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MAX_LEN = 1024\n",
    "\n",
    "\n",
    "def build_prompt_text(instruction, inp=''):\n",
    "    fixed_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'\n",
    "    instruction = instruction.strip()\n",
    "    inp = inp.strip() if inp else ''\n",
    "    if inp:\n",
    "        return (fixed_prompt + \"\\n\\n\" +\n",
    "                \"### Instruction:\\n\" + instruction + \"\\n\\n\" +\n",
    "                \"### Input:\\n\" + inp + \"\\n\\n\" +\n",
    "                \"### Response:\\n\")\n",
    "    else:\n",
    "        return (fixed_prompt + \"\\n\\n\" +\n",
    "                \"### Instruction:\\n\" + instruction + \"\\n\\n\" +\n",
    "                \"### Response:\\n\")\n",
    "\n",
    "\n",
    "# Dataset that tokenizes Alpaca examples and builds masked targets\n",
    "class AlpacaDataset(Dataset):\n",
    "    def __init__(self, raw_dataset):\n",
    "        self.raw = raw_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.raw[idx]\n",
    "        instr = ex.get('instruction', '').strip()\n",
    "        inp = ex.get('input', '').strip()\n",
    "        out = ex.get('output', '').strip()\n",
    "        prompt_text = build_prompt_text(instr, inp)\n",
    "        prompt_ids = tokenizer(\n",
    "            prompt_text,\n",
    "            add_special_tokens=False,\n",
    "            max_length=MAX_LEN,\n",
    "            truncation=True\n",
    "        )['input_ids']\n",
    "        response_ids = tokenizer(\n",
    "            out,\n",
    "            add_special_tokens=False,\n",
    "            max_length=MAX_LEN,\n",
    "            truncation=True\n",
    "        )['input_ids']\n",
    "        # Compose sequence: prompt + response + eos\n",
    "        ids = prompt_ids + response_ids + [tokenizer.eos_token_id]\n",
    "        ids = ids[:MAX_LEN]\n",
    "        input_ids = torch.tensor(ids[:-1], dtype=torch.long)\n",
    "        targets = torch.tensor(ids[1:], dtype=torch.long)\n",
    "        # Mask: ignore all target positions before the response start\n",
    "        masked_targets = targets.clone()\n",
    "        prefix_len = len(prompt_ids)\n",
    "        ignore_until = max(0, min(prefix_len, len(masked_targets)))\n",
    "        if ignore_until > 0:\n",
    "            masked_targets[:ignore_until] = -100\n",
    "        return input_ids, masked_targets\n",
    "\n",
    "\n",
    "# Instantiate dataset\n",
    "train_ds = AlpacaDataset(dataset)\n",
    "\n",
    "\n",
    "# Collate function pads inputs and targets\n",
    "def collate_batch(examples):\n",
    "    inputs = [ex[0] for ex in examples]\n",
    "    targets = [ex[1] for ex in examples]\n",
    "    inputs_pad = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    targets_pad = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=-100)\n",
    "    return inputs_pad, targets_pad\n",
    "\n",
    "\n",
    "# Sort indices by sequence length to minimise padding\n",
    "def length_sorted_indices(dataset):\n",
    "    lengths = [(i, len(dataset[i][0])) for i in range(len(dataset))]\n",
    "    lengths.sort(key=lambda x: x[1])\n",
    "    return [i for i, _ in lengths]\n",
    "\n",
    "\n",
    "sorted_idx = length_sorted_indices(train_ds)\n",
    "sampler = SequentialSampler(sorted_idx)\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=0\n",
    ")"
   ],
   "id": "17f2b5e976bf00e4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T09:56:57.680204Z",
     "start_time": "2025-09-06T09:56:57.667205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inspect a sample\n",
    "sample_inputs, sample_targets = train_ds[0]\n",
    "print(tokenizer.decode(sample_inputs.tolist()))"
   ],
   "id": "ed4a95bc92c8db3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T09:56:58.039663Z",
     "start_time": "2025-09-06T09:56:57.684716Z"
    }
   },
   "source": [
    "# Model hyperparameters\n",
    "vocab_size = len(tokenizer)\n",
    "d_model = 384\n",
    "n_layers = 6\n",
    "n_heads = 4\n",
    "d_state = d_model // n_heads\n",
    "dropout = 0.1\n",
    "\n",
    "model = MambaLM(vocab_size, d_model, n_layers, n_heads, d_state, dropout)\n",
    "# Resize embeddings if tokenizer has grown\n",
    "model.emb = nn.Embedding(vocab_size, d_model)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and criterion (masked loss)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)"
   ],
   "id": "2fc31fb9496d4d15",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T11:34:42.280618Z",
     "start_time": "2025-09-06T09:56:58.044667Z"
    }
   },
   "source": [
    "epochs = 5\n",
    "log_interval = 100\n",
    "sample_length = 100\n",
    "\n",
    "# TensorBoard writer\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "save_dir = f'runs/mamba_{timestamp}'\n",
    "writer = SummaryWriter(log_dir=save_dir)\n",
    "\n",
    "step = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "fixed_prompt = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\nTell a bedtime story about a dragon and a little village.\\n\\n\"\n",
    "    \"### Response:\\n\"\n",
    ")\n",
    "fixed_prompt_ids = tokenizer(\n",
    "    fixed_prompt,\n",
    "    return_tensors='pt',\n",
    "    max_length=MAX_LEN,\n",
    "    truncation=True\n",
    ")['input_ids'].to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_total_loss = 0.0\n",
    "    epoch_batches = 0\n",
    "    model.train()\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        logits, _ = model(inputs)\n",
    "        loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        loss_value = loss.item()\n",
    "        epoch_total_loss += loss_value\n",
    "        epoch_batches += 1\n",
    "        writer.add_scalar('loss/train_step', loss_value, step)\n",
    "        step += 1\n",
    "        # Logging and sample generation\n",
    "        if step % log_interval == 0:\n",
    "            avg_loss = epoch_total_loss / epoch_batches\n",
    "            writer.add_scalar('loss/train_avg', avg_loss, step)\n",
    "            # Save model checkpoint\n",
    "            ckpt = {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"config\": {\n",
    "                    \"vocab_size\": model.emb.num_embeddings,\n",
    "                    \"d_model\": model.emb.embedding_dim,\n",
    "                    \"n_layers\": model.n_layers,\n",
    "                    \"n_heads\": model.n_heads,\n",
    "                    \"d_state\": model.d_state,\n",
    "                    \"dropout\": model.dropout.p,\n",
    "                },\n",
    "            }\n",
    "            torch.save(ckpt, f'{save_dir}/last.pt')\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                torch.save(ckpt, f'{save_dir}/best.pt')\n",
    "            # reset running averages\n",
    "            epoch_total_loss = 0.0\n",
    "            epoch_batches = 0\n",
    "            # Sample a continuation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                gen_ids = model.generate(fixed_prompt_ids, max_new_tokens=sample_length)\n",
    "            new_tokens = gen_ids[0][fixed_prompt_ids.size(1):]\n",
    "            sample_text = tokenizer.decode(new_tokens.tolist(), skip_special_tokens=True)\n",
    "            print('\\n--- Sample Generation ---')\n",
    "            print(fixed_prompt + sample_text)\n",
    "            print('-------------------------\\n')\n",
    "            writer.add_text('samples', f'{fixed_prompt}{sample_text}', step)\n",
    "            print(f'Step {step}, Loss: {avg_loss:.4f}')\n",
    "            model.train()\n",
    "    print(f'Epoch {epoch + 1} complete')\n",
    "writer.close()"
   ],
   "id": "a334e39ff4b2452b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/13001 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e044651d96394e709a2ba1845d8e9022"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, Loss: 9.4397\n",
      "Step 200, Loss: 7.3196\n",
      "Step 300, Loss: 6.9532\n",
      "Step 400, Loss: 6.8303\n",
      "Step 500, Loss: 6.5756\n",
      "Step 600, Loss: 6.6200\n",
      "Step 700, Loss: 6.4067\n",
      "Step 800, Loss: 6.3598\n",
      "Step 900, Loss: 6.3909\n",
      "Step 1000, Loss: 6.2738\n",
      "Step 1100, Loss: 6.1548\n",
      "Step 1200, Loss: 6.2005\n",
      "Step 1300, Loss: 6.0586\n",
      "Step 1400, Loss: 6.1522\n",
      "Step 1500, Loss: 5.8736\n",
      "Step 1600, Loss: 6.0656\n",
      "Step 1700, Loss: 5.8670\n",
      "Step 1800, Loss: 5.8462\n",
      "Step 1900, Loss: 5.8997\n",
      "Step 2000, Loss: 5.7596\n",
      "Step 2100, Loss: 5.7372\n",
      "Step 2200, Loss: 5.6675\n",
      "Step 2300, Loss: 5.7551\n",
      "Step 2400, Loss: 5.7898\n",
      "Step 2500, Loss: 5.6826\n",
      "Step 2600, Loss: 5.5486\n",
      "Step 2700, Loss: 5.6033\n",
      "Step 2800, Loss: 5.5984\n",
      "Step 2900, Loss: 5.5981\n",
      "Step 3000, Loss: 5.5244\n",
      "Step 3100, Loss: 5.5728\n",
      "Step 3200, Loss: 5.5372\n",
      "Step 3300, Loss: 5.4833\n",
      "Step 3400, Loss: 5.4443\n",
      "Step 3500, Loss: 5.3376\n",
      "Step 3600, Loss: 5.4330\n",
      "Step 3700, Loss: 5.4123\n",
      "Step 3800, Loss: 5.5098\n",
      "Step 3900, Loss: 5.4868\n",
      "Step 4000, Loss: 5.4145\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 31\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, targets \u001B[38;5;129;01min\u001B[39;00m tqdm(train_loader):\n\u001B[0;32m     30\u001B[0m     inputs, targets \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m), targets\u001B[38;5;241m.\u001B[39mto(device, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 31\u001B[0m     logits, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(logits\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, vocab_size), targets\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m     33\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Documents\\PyCharm\\Mamba\\lib\\mamba2.py:250\u001B[0m, in \u001B[0;36mMambaLM.forward\u001B[1;34m(self, tokens, states, return_state)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# Pass through blocks\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers):\n\u001B[1;32m--> 250\u001B[0m     x, new_state \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstates\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    251\u001B[0m     new_states\u001B[38;5;241m.\u001B[39mappend(new_state)\n\u001B[0;32m    253\u001B[0m \u001B[38;5;66;03m# Final normalization and projection\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Documents\\PyCharm\\Mamba\\lib\\mamba2.py:149\u001B[0m, in \u001B[0;36mMambaBlock.forward\u001B[1;34m(self, x, state)\u001B[0m\n\u001B[0;32m    146\u001B[0m ssm_out \u001B[38;5;241m=\u001B[39m C_t \u001B[38;5;241m*\u001B[39m h\n\u001B[0;32m    148\u001B[0m \u001B[38;5;66;03m# Reshape ssm_out to (batch, d_model) by concatenating heads\u001B[39;00m\n\u001B[1;32m--> 149\u001B[0m ssm_out \u001B[38;5;241m=\u001B[39m \u001B[43mssm_out\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbsz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;66;03m# Mix SSM output with the original input via gating\u001B[39;00m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;66;03m# We cast gate_t to the same shape (batch, d_model)\u001B[39;00m\n\u001B[0;32m    153\u001B[0m mixed \u001B[38;5;241m=\u001B[39m gate_t \u001B[38;5;241m*\u001B[39m ssm_out \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m-\u001B[39m gate_t) \u001B[38;5;241m*\u001B[39m x_norm[:, t]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
